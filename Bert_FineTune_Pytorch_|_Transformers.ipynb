{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4F6UFLLnba7v/+RsWmfcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diwakarojha/AI_Machine_Learning/blob/main/Bert_FineTune_Pytorch_%7C_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT QnA: https://stackabuse.com/guide-to-fine-tuning-open-source-llms-on-custom-data/\n",
        "\n",
        "GPT2: https://metatext.io/blog/how-to-finetune-llm-hugging-face-transformers\n",
        "\n",
        "Finetune LLM: https://huggingface.co/docs/transformers/notebooks"
      ],
      "metadata": {
        "id": "GxkrJOl0O88K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmpj6UxKwDbR",
        "outputId": "200b900f-ba76-4d2b-eb98-e4ba8ec8a7c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-TQduPicO-R9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tHz_cRfSvoro"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForQuestionAnswering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "2y700C2AyBMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class diabetes(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = self.load_data(file_path)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        paragraphs = data['data'][0]['paragraphs']\n",
        "        extracted_data = []\n",
        "        for paragraph in paragraphs:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answer = qa['answers'][0]['text']\n",
        "                start_pos = qa['answers'][0]['answer_start']\n",
        "                extracted_data.append({\n",
        "                    'context': context,\n",
        "                    'question': question,\n",
        "                    'answer': answer,\n",
        "                    'start_pos': start_pos,\n",
        "                })\n",
        "        return extracted_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.data[index]\n",
        "        question = example['question']\n",
        "        context = example['context']\n",
        "        answer = example['answer']\n",
        "        inputs = self.tokenizer.encode_plus(question, context, add_special_tokens=True, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
        "        input_ids = inputs['input_ids'].squeeze()\n",
        "        attention_mask = inputs['attention_mask'].squeeze()\n",
        "        start_pos = torch.tensor(example['start_pos'])\n",
        "        return input_ids, attention_mask, start_pos, end_pos"
      ],
      "metadata": {
        "id": "Gy-Y-cK8Ovjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the custom dataset\n",
        "file_path = 'diabetes.json'\n",
        "dataset = diabetes(file_path)"
      ],
      "metadata": {
        "id": "Wou6hkfhOyKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the BERT model for question answering\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "batch_size = 8\n",
        "num_epochs = 50\n",
        "\n",
        "# Create data loader\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "wnsf3OCkO08q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        # Move batch tensors to the device\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        start_positions = batch[2].to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "gxG91x8IO4Hm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}